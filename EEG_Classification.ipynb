{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEG_Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jabarragann/PytorchTutorialsJuanAntonio/blob/master/EEG_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH0rzpJvZdj8",
        "colab_type": "text"
      },
      "source": [
        "# Connecting to Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrzQqrG9YtTW",
        "colab_type": "code",
        "outputId": "62cb231c-e4c1-4b7e-954a-ae584783d6c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngYj70w1ZOOE",
        "colab_type": "code",
        "outputId": "fc98da7a-6328-4c4a-df96-075f32746f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# !ls\n",
        "# !mkdir -p 'gdrive/My Drive/ToshibaJuanAntonio/Trabajos Purdue/Surgical Intuitive Grant/MentalWorkloadPrediction/Data/D4/'\n",
        "# !mkdir -p 'gdrive/My Drive/ToshibaJuanAntonio/Trabajos Purdue/Surgical Intuitive Grant/MentalWorkloadPrediction/Data/D4/preprocess'\n",
        "!ls\n",
        "import os\n",
        "os.chdir('/content')\n",
        "!ls \n",
        "\n",
        "#!pip install pycuda"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tsample_data\n",
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9QDqr3Iw93H",
        "colab_type": "text"
      },
      "source": [
        "#Setting up the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osiCHsR22VSf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "1. Delete current run time directory\n",
        "2. Create all needed directories\n",
        "3. Copy data from google drive account to Colab directory\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rM_oxsX0CHc",
        "colab_type": "code",
        "outputId": "1bf93ef6-257f-43b7-bb62-ae768144ac22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def createDirectories(rootPath, *args):\n",
        "      \n",
        "    print(\"Creating directories\")\n",
        "    for idx, path in enumerate(args):\n",
        "        try:  \n",
        "            os.makedirs(rootPath + path)\n",
        "        except OSError:  \n",
        "            print (\"{:d}: Creation of the directory {} failed\".format(idx,path))\n",
        "        else:  \n",
        "            print (\"{:d}: Successfully created the directory {}\".format(idx, path))\n",
        "    print()\n",
        "\n",
        "def copyFiles(srcRootPath,dstRootPath, *directories):\n",
        "    print(\"copy data from google drive\")\n",
        "    for idx, path in enumerate(directories):\n",
        "        print(\"{:d}: Copy files from: {:s}\".format(idx,path) )\n",
        "        completePath = srcRootPath+path\n",
        "        srcFiles = os.listdir(completePath)\n",
        "        srcFiles.sort()\n",
        "\n",
        "        for file in srcFiles:\n",
        "            srcPath = completePath + file\n",
        "            dstPath = dstRootPath+path\n",
        "\n",
        "            if os.path.isfile(srcPath):\n",
        "                shutil.copy(srcPath, dstPath)\n",
        "\n",
        "    print()\n",
        "\n",
        "#Set /content as working directory\n",
        "os.chdir('/content')\n",
        "\n",
        "#Remove Previous RuntimeDirectory\n",
        "try:\n",
        "    shutil.rmtree('./RunTimeDir')\n",
        "except:\n",
        "    print(\"RunTimeDirectory not found\")\n",
        "else:\n",
        "    print(\"Deleting previous RunTimeDirectory\")\n",
        "finally:\n",
        "    print()\n",
        "\n",
        "\n",
        "# Create new directories\n",
        "rootPath = \"./RunTimeDir/Data/\"\n",
        "directories = ['D1/preprocessed','D1/final', 'D1/raw',\n",
        "               'D2/preprocessed','D2/final', 'D2/raw',\n",
        "               'D3/preprocessed','D3/final', 'D3/raw',\n",
        "               'D4/preprocessed','D4/final', 'D4/raw',]\n",
        "\n",
        "createDirectories(rootPath, *directories)\n",
        "\n",
        "\n",
        "#Copy data from google drive to RuntimeDir\n",
        "gdrivePath ='gdrive/My Drive/ToshibaJuanAntonio/Trabajos Purdue/Surgical Intuitive Grant/MentalWorkloadPrediction/Data/'\n",
        "srcDirectories = ['D1/raw/', 'D2/raw/', 'D3/raw/','D4/raw/']\n",
        "\n",
        "dstRootPath = \"./RunTimeDir/Data/\"\n",
        "\n",
        "copyFiles(gdrivePath, dstRootPath, *srcDirectories)\n",
        "\n",
        "\n",
        "# Check current working directory\n",
        "os.chdir('/content/RunTimeDir')\n",
        "path = os.getcwd()  \n",
        "print (\"The current working directory is %s\" % path, end='\\n\\n') \n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RunTimeDirectory not found\n",
            "\n",
            "Creating directories\n",
            "0: Successfully created the directory D1/preprocessed\n",
            "1: Successfully created the directory D1/final\n",
            "2: Successfully created the directory D1/raw\n",
            "3: Successfully created the directory D2/preprocessed\n",
            "4: Successfully created the directory D2/final\n",
            "5: Successfully created the directory D2/raw\n",
            "6: Successfully created the directory D3/preprocessed\n",
            "7: Successfully created the directory D3/final\n",
            "8: Successfully created the directory D3/raw\n",
            "9: Successfully created the directory D4/preprocessed\n",
            "10: Successfully created the directory D4/final\n",
            "11: Successfully created the directory D4/raw\n",
            "\n",
            "copy data from google drive\n",
            "0: Copy files from: D1/raw/\n",
            "1: Copy files from: D2/raw/\n",
            "2: Copy files from: D3/raw/\n",
            "3: Copy files from: D4/raw/\n",
            "\n",
            "The current working directory is /content/RunTimeDir\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCqk-GWaKBWA",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UgdCehtHgzq",
        "colab_type": "text"
      },
      "source": [
        "Fuse Timestamps with sensor data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDQ64HbkHkdq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "bab007af-a3bc-425c-d170-8da136053b31"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product\n",
        "import shutil\n",
        "\n",
        "\n",
        "def preprocessData(trial=None, dataset=None):\n",
        "    print(\"Preprocess Data from trial {:d}\".format(trial))\n",
        "\n",
        "    #Read Raw files\n",
        "    epoc = pd.read_csv(\"./Data/D{:d}/raw/S1_T{:d}_Epoc.txt\".format(dataset,trial), sep=' ')\n",
        "    shimmer = pd.read_csv(\"./Data/D{:d}/raw/S1_T{:d}_Shimmer.txt\".format(dataset,trial), sep=' ')\n",
        "    secondaryTaskStamps = pd.read_csv(\"./Data/D{:d}/raw/S1_T{:d}_Timestamps.txt\".format(dataset, trial), sep=' ', skiprows=[0,1,2,3])\n",
        "    \n",
        "    fiveSecondStampsSrc =\"./Data/D{:d}/raw/S1_T{:d}_TimestampEvery5Seconds.txt\".format(dataset,trial)\n",
        "    fiveSecondStampsDst =\"./Data/D{:d}/preprocessed/S1_T{:d}_TimestampEvery5Seconds.txt\".format(dataset,trial)\n",
        "    \n",
        "    fiveSecondWindowStamps = pd.read_csv(fiveSecondStampsSrc, sep=' ')\n",
        "    \n",
        "    #Copy 5seconds timestamp to preprocessed file\n",
        "    shutil.copy(fiveSecondStampsSrc, fiveSecondStampsDst)\n",
        "    \n",
        "    #Get Data Columns\n",
        "    epocSlice = epoc[['AF3', 'F7', 'F3', 'FC5', 'T7',\n",
        "                      'P7', 'O1', 'O2', 'P8', 'T8',\n",
        "                      'FC6', 'F4', 'F8', 'AF4', 'COMPUTER_TIME']]\n",
        "    epocArr = epocSlice.values\n",
        "\n",
        "    shimmerSlice = shimmer[['PPG','COMPUTER_TIME']]\n",
        "    shimmerArr = shimmerSlice.values\n",
        "\n",
        "    secondaryTaskArr = secondaryTaskStamps.values\n",
        "    windowStampsArr = fiveSecondWindowStamps.values\n",
        "\n",
        "    finalArrayEpoc = np.ones((epocArr.shape[0], epocArr.shape[1] + 3)) * -10\n",
        "    finalArrayEpoc[:, list(range(15))] = epocArr\n",
        "\n",
        "    finalArrayShimmer = np.ones((shimmerArr.shape[0], shimmerArr.shape[1] + 3)) * (-10)\n",
        "    finalArrayShimmer[:, list(range(2))] = shimmerArr\n",
        "\n",
        "    #Apply a band pass Butterworth Filter to each of the 14 channels of the EEG\n",
        "    from scipy.signal import butter, lfilter\n",
        "\n",
        "    def butter_bandpass(lowcut, highcut, fs, order=7):\n",
        "        nyq = 0.5 * fs\n",
        "        low = lowcut / nyq\n",
        "        high = highcut / nyq\n",
        "        b, a = butter(order, [low, high], btype='band')\n",
        "        return b, a\n",
        "\n",
        "    def butter_bandpass_filter(data, lowcut, highcut, fs, order=7):\n",
        "        b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "        y = lfilter(b, a, data)\n",
        "        return y\n",
        "\n",
        "    #EPOC Filtering\n",
        "    lowCut  = 0.5\n",
        "    highCut = 50\n",
        "    for i in range(14):\n",
        "        temp = butter_bandpass_filter(finalArrayEpoc[:, i], lowCut, highCut, 128, order=7)\n",
        "        finalArrayEpoc[:, i] = temp\n",
        "\n",
        "    #Shimmer Filtering\n",
        "    temp = butter_bandpass_filter(finalArrayShimmer[:, 0], lowCut, highCut, 128, order=7)\n",
        "    finalArrayShimmer[:, 0] = temp\n",
        "\n",
        "    #Remove 10 first seconds of data to take away outliers and transient state created by the band pass filter\n",
        "    tempFinal = np.ones((epocArr.shape[0] - 700 , epocArr.shape[1]+3)) * -10\n",
        "    tempFinal = finalArrayEpoc[700:, :]\n",
        "    finalArrayEpoc = tempFinal\n",
        "\n",
        "    tempFinal = np.ones((shimmerArr.shape[0] - 700 , shimmerArr.shape[1]+3)) * -10\n",
        "    tempFinal = finalArrayShimmer[700:, :]\n",
        "    finalArrayShimmer = tempFinal\n",
        "\n",
        "    #### EEG PRE PROCESSING ####\n",
        "    #Add Secondary task timestamps to EEG data\n",
        "    idx = 0\n",
        "    for i in range(secondaryTaskArr.shape[0]):\n",
        "        timestamp = secondaryTaskArr[i,0]\n",
        "        while timestamp > epocArr[idx,14]:\n",
        "            idx += 1\n",
        "            if idx == finalArrayEpoc.shape[0]:\n",
        "                break\n",
        "\n",
        "        if idx == finalArrayEpoc.shape[0]:\n",
        "            break\n",
        "        if idx < finalArrayEpoc.shape[0]:\n",
        "            finalArrayEpoc[idx, -3] = timestamp\n",
        "\n",
        "    #Add 5 Seconds Windows timestamps to EEG data\n",
        "    idx = 0\n",
        "    for i in range(windowStampsArr.shape[0]):\n",
        "        timestamp = windowStampsArr[i,0]\n",
        "        while timestamp > epocArr[idx,14]:\n",
        "            idx += 1\n",
        "            if idx == finalArrayEpoc.shape[0]:\n",
        "                break\n",
        "\n",
        "        if idx == finalArrayEpoc.shape[0]:\n",
        "            break\n",
        "        if idx < finalArrayEpoc.shape[0]:\n",
        "            finalArrayEpoc[idx, -2] = timestamp\n",
        "\n",
        "    # Add Label\n",
        "    count = 0\n",
        "    for idx in range(finalArrayEpoc.shape[0]):\n",
        "        time = finalArrayEpoc[idx, 14]\n",
        "\n",
        "        if count == secondaryTaskArr.shape[0] -1:\n",
        "            finalArrayEpoc[idx, -1] = 1 if secondaryTaskArr[count, 1] is True else 0\n",
        "        else:\n",
        "            if time > secondaryTaskArr[count,0] and  time < secondaryTaskArr[count+1,0]:\n",
        "                finalArrayEpoc[idx, -1] = 1 if secondaryTaskArr[count, 1] is True else 0\n",
        "            elif time > secondaryTaskArr[count+1, 0]:\n",
        "                finalArrayEpoc[idx, -1] = 1 if secondaryTaskArr[count + 1, 1] is True else 0\n",
        "                count += 1\n",
        "\n",
        "\n",
        "    #### SHIMMER PRE PROCESSING ####\n",
        "    #Add Secondary task timestamps to EEG data\n",
        "    idx = 0\n",
        "    for i in range(secondaryTaskArr.shape[0]):\n",
        "        timestamp = secondaryTaskArr[i,0]\n",
        "        while timestamp > shimmerArr[idx,1]:\n",
        "            idx += 1\n",
        "            if idx == finalArrayShimmer.shape[0]:\n",
        "                break\n",
        "\n",
        "        if idx == finalArrayShimmer.shape[0]:\n",
        "            break\n",
        "        if idx < finalArrayShimmer.shape[0]:\n",
        "            finalArrayShimmer[idx, -3] = timestamp\n",
        "\n",
        "    #Add 5 Seconds Windows timestamps to Shimmer data\n",
        "    idx = 0\n",
        "    for i in range(windowStampsArr.shape[0]):\n",
        "        timestamp = windowStampsArr[i,0]\n",
        "        while timestamp > shimmerArr[idx,1]:\n",
        "            idx += 1\n",
        "            if idx == finalArrayShimmer.shape[0]:\n",
        "                break\n",
        "\n",
        "        if idx == finalArrayShimmer.shape[0]:\n",
        "            break\n",
        "        if idx < finalArrayShimmer.shape[0]:\n",
        "            finalArrayShimmer[idx, -2] = timestamp\n",
        "\n",
        "    # Add Label\n",
        "    count = 0\n",
        "    for idx in range(finalArrayShimmer.shape[0]):\n",
        "        time = finalArrayShimmer[idx, 1]\n",
        "\n",
        "        if count == secondaryTaskArr.shape[0] -1:\n",
        "            finalArrayShimmer[idx, -1] = 1 if secondaryTaskArr[count, 1] is True else 0\n",
        "        else:\n",
        "            if time > secondaryTaskArr[count,0] and  time < secondaryTaskArr[count+1,0]:\n",
        "                finalArrayShimmer[idx, -1] = 1 if secondaryTaskArr[count, 1] is True else 0\n",
        "            elif time > secondaryTaskArr[count+1, 0]:\n",
        "                finalArrayShimmer[idx, -1] = 1 if secondaryTaskArr[count + 1, 1] is True else 0\n",
        "                count += 1\n",
        "\n",
        "    # Write Final File\n",
        "    #EEG File\n",
        "    with open('./Data/D{:d}/preprocessed/S1_T{:d}_fusion_epoc.txt'.format(dataset, trial),'w') as fout:\n",
        "        fout.write(\"AF3 F7 F3 FC5 T7 P7 O1 O2 P8 T8 FC6 F4 F8 AF4 COMPUTER_TIME SECONDARY_TASK 5_SECOND_WINDOW LABEL\\n\")\n",
        "        for i in range(finalArrayEpoc.shape[0]):\n",
        "            formattedData = \" \".join([\"{:.8f}\".format(d) for d in finalArrayEpoc[i]])\n",
        "            fout.write(formattedData+'\\n')\n",
        "    \n",
        "    #Shimmer File\n",
        "    with open('./Data/D{:d}/preprocessed/S1_T{:d}_fusion_shimmer.txt'.format(dataset, trial),'w') as fout:\n",
        "        fout.write(\"PPG COMPUTER_TIME SECONDARY_TASK 5_SECOND_WINDOW LABEL\\n\")\n",
        "        for i in range(finalArrayShimmer.shape[0]):\n",
        "            formattedData = \" \".join([\"{:.8f}\".format(d) for d in finalArrayShimmer[i]])\n",
        "            fout.write(formattedData+'\\n')\n",
        "            \n",
        "########            \n",
        "# main##\n",
        "########\n",
        "\n",
        "d1 = product([1],[3,4,5,6,7])\n",
        "d2 = product([2],[2,4,5,6,7])\n",
        "d3 = product([3],[3,4,5,6,7])\n",
        "d4 = product([4],[1,2,3,4,5])\n",
        "\n",
        "completeData = [d1,d2, d3,d4]\n",
        "\n",
        "for idx, iterT in enumerate(completeData):\n",
        "    print(\"Preprocessing dataset {:d}\".format(idx + 1))\n",
        "    for dataset, trial in iterT:\n",
        "        preprocessData(trial=trial, dataset=dataset)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing dataset 1\n",
            "Preprocess Data from trial 3\n",
            "Preprocess Data from trial 4\n",
            "Preprocess Data from trial 5\n",
            "Preprocess Data from trial 6\n",
            "Preprocess Data from trial 7\n",
            "Preprocessing dataset 2\n",
            "Preprocess Data from trial 2\n",
            "Preprocess Data from trial 4\n",
            "Preprocess Data from trial 5\n",
            "Preprocess Data from trial 6\n",
            "Preprocess Data from trial 7\n",
            "Preprocessing dataset 3\n",
            "Preprocess Data from trial 3\n",
            "Preprocess Data from trial 4\n",
            "Preprocess Data from trial 5\n",
            "Preprocess Data from trial 6\n",
            "Preprocess Data from trial 7\n",
            "Preprocessing dataset 4\n",
            "Preprocess Data from trial 1\n",
            "Preprocess Data from trial 2\n",
            "Preprocess Data from trial 3\n",
            "Preprocess Data from trial 4\n",
            "Preprocess Data from trial 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpCAvOJ5Hmv-",
        "colab_type": "text"
      },
      "source": [
        "#Create Pickle files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMQ3lIgtaArT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "437658f9-fae8-4a19-f9e8-4b19a98e3706"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "from scipy import signal\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "class DataContainer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.dataWindowArray = []\n",
        "        self.length = 0\n",
        "\n",
        "        # Normalizing values of whole trial\n",
        "        self.epocMean = -1\n",
        "        self.epocStd = -1\n",
        "        self.shimmerMean = -1\n",
        "        self.shimmerStd = -1\n",
        "\n",
        "    def createWindow(self,beginTime, endTime, shimmerSize, epocSize, shimmerScaling=None,\n",
        "                                                                     epocScaling=None):\n",
        "        self.dataWindowArray.append(DataWindow(beginTime, endTime, shimmerSize, epocSize,shimmerScaling, epocScaling))\n",
        "        self.length += 1\n",
        "\n",
        "    def fillData(self, shimmerData=None, epocData=None):\n",
        "        #Fill Shimmer Data\n",
        "        windowIdx = 0\n",
        "        shimmerIdx = 0\n",
        "        shimmerData = shimmerData[['PPG', 'COMPUTER_TIME', 'LABEL']].values\n",
        "        for i in range(shimmerData.shape[0]):\n",
        "            while shimmerData[i,-2] > self.dataWindowArray[windowIdx].endTime:\n",
        "                windowIdx += 1\n",
        "                shimmerIdx = 0\n",
        "\n",
        "                if windowIdx >= self.length:\n",
        "                    break\n",
        "\n",
        "            if windowIdx >= self.length:\n",
        "                break\n",
        "\n",
        "            self.dataWindowArray[windowIdx].shimmerArray[shimmerIdx] = shimmerData[i,:]\n",
        "            self.dataWindowArray[windowIdx].actualShimmerSize += 1\n",
        "            shimmerIdx += 1\n",
        "\n",
        "        #Fill Epoc Data\n",
        "        windowIdx = 0\n",
        "        epocIdx = 0\n",
        "        epocData = epocData[['AF3', 'F7', 'F3', 'FC5', 'T7',\n",
        "                             'P7', 'O1', 'O2', 'P8', 'T8',\n",
        "                             'FC6', 'F4', 'F8', 'AF4', 'COMPUTER_TIME',\n",
        "                             'LABEL']].values\n",
        "       \n",
        "                    \n",
        "        for i in range(epocData.shape[0]):\n",
        "          \n",
        "            '''\n",
        "            Get the correct initial window (This needs to be done because the secondary \n",
        "            task labels started before that the sensor streaming of data)\n",
        "            '''\n",
        "            while epocData[i,-2] > self.dataWindowArray[windowIdx].endTime:\n",
        "                windowIdx += 1\n",
        "                epocIdx = 0\n",
        "            \n",
        "                if windowIdx >= self.length:\n",
        "                    break\n",
        "            \n",
        "            \n",
        "            if windowIdx >= self.length:\n",
        "                break\n",
        "            \n",
        "            '''\n",
        "            Discard data that was taken before the first window of time\n",
        "            '''\n",
        "            if epocData[i,-2] > self.dataWindowArray[windowIdx].beginTime:\n",
        "                self.dataWindowArray[windowIdx].epocArray[epocIdx] = epocData[i,:]\n",
        "                self.dataWindowArray[windowIdx].actualEpocSize += 1\n",
        "                epocIdx += 1\n",
        "\n",
        "    def createMetrics(self):\n",
        "            \n",
        "        for i in range(self.length):\n",
        "            self.dataWindowArray[i].calculateLabel()\n",
        "            self.dataWindowArray[i].createSpectogramVolume()\n",
        "\n",
        "    def normalizeData(self):\n",
        "        for i in range(self.length):\n",
        "            self.dataWindowArray[i].normalizeData()\n",
        "\n",
        "    def dumpWindowsToFiles(self, destinationPath, trial):\n",
        "        for i in range(self.length):\n",
        "            self.dataWindowArray[i].createPickleFile(i, destinationPath, trial)\n",
        "\n",
        "\n",
        "\n",
        "class DataWindow:\n",
        "\n",
        "    def __init__(self, beginTime, endTime, shimmerSize, epocSize,shimmerScaling=None, epocScaling=None):\n",
        "\n",
        "        self.beginTime = float(beginTime)\n",
        "        self.endTime = float(endTime)\n",
        "\n",
        "        self.shimmerArray = np.zeros((shimmerSize, 1+2))\n",
        "        self.epocArray = np.zeros((epocSize, 14+2))\n",
        "        self.actualShimmerSize = 0\n",
        "        self.actualEpocSize = 0\n",
        "\n",
        "        self.spectogramVolume = np.zeros((14,51,7))\n",
        "        self.globalLabel = None\n",
        "\n",
        "        self.shimmerMean = shimmerScaling[0]\n",
        "        self.shimmerStd = shimmerScaling[1]\n",
        "        self.epocMean = epocScaling[0]\n",
        "        self.epocStd = epocScaling[1]\n",
        "\n",
        "\n",
        "    def createSpectogramVolume(self):\n",
        "        if self.actualEpocSize > 400:\n",
        "            #Add EEG Spectogram\n",
        "            for j in range(14):\n",
        "                x = self.epocArray[:self.actualEpocSize, j]\n",
        "                f, t, Sxx = signal.spectrogram(x, 1, nperseg=100, mode='magnitude')\n",
        "                # print(Sxx.max())\n",
        "                Sxx = cv2.resize(Sxx, dsize=(7, 51))\n",
        "                self.spectogramVolume[j, :, :] = Sxx\n",
        "\n",
        "            # #Add PPG Spectogram\n",
        "            # if self.actualShimmerSize < 600:\n",
        "            #     print(self.actualShimmerSize)\n",
        "            #\n",
        "            # x = self.shimmerArray[:self.actualShimmerSize, 0]\n",
        "            # f, t, Sxx = signal.spectrogram(x, 1, nperseg=100, mode='magnitude')\n",
        "            # Sxx = cv2.resize(Sxx, dsize=(7, 51))\n",
        "            # self.spectogramVolume[14, :, :] = Sxx\n",
        "\n",
        "\n",
        "    def normalizeData(self):\n",
        "        self.epocArray[:,:14] = (self.epocArray[:,:14] - self.epocMean)/self.epocStd\n",
        "        self.shimmerArray[:,0] = (self.shimmerArray[:,0] - self.shimmerMean)/self.shimmerStd\n",
        "\n",
        "    def calculateLabel(self):\n",
        "        totalLength = self.actualEpocSize + 1e-6\n",
        "        label = sum(self.epocArray[:self.actualEpocSize, -1]) / totalLength\n",
        "        self.globalLabel = round(label)\n",
        "\n",
        "    def createPickleFile(self, windowIdx, destinationPath, trial):\n",
        "        if self.actualEpocSize > 400 and windowIdx>4:\n",
        "            finalDict = {'data': self.spectogramVolume, 'label': self.globalLabel}\n",
        "            with open(destinationPath + 'S1_T{:d}_{:03d}.pickle'.format(trial, windowIdx), 'wb') as handle:\n",
        "                pickle.dump(finalDict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "TRIAL = 7\n",
        "DATASET = 1\n",
        "import os\n",
        "\n",
        "d1 = (1,[3,4,5,6,7])\n",
        "d2 = (2,[2,4,5,6,7])\n",
        "d3 = (3,[3,4,5,6,7])\n",
        "d4 = (4,[1,2,3,4,5])\n",
        "completeData = [d1,d2,d3,d4]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "    os.chdir('/content/RunTimeDir')\n",
        "    path = os.getcwd()  \n",
        "    print (\"The current working directory is %s\" % path, end='\\n\\n') \n",
        "    \n",
        "    for DATASET, TRIALS in completeData:\n",
        "        print(\"Final preprocessing of D{:d}\".format(DATASET))\n",
        "        \n",
        "        destinationPath = \"./Data/D{:d}/final/\".format(DATASET)\n",
        "        dataPath = \"./Data/D{:d}/preprocessed/\".format(DATASET)\n",
        "\n",
        "        try:\n",
        "            os.mkdir(destinationPath)\n",
        "        except OSError:\n",
        "            print(\"Creation of the directory %s failed\" % destinationPath)\n",
        "        else:\n",
        "            print(\"Successfully created the directory %s \" % destinationPath)\n",
        "\n",
        "\n",
        "        for TRIAL in TRIALS:\n",
        "            shimmerFile = pd.read_csv(dataPath + 'S1_T{:d}_fusion_shimmer.txt'.format(TRIAL), sep=' ')\n",
        "            epocFile = pd.read_csv(dataPath + 'S1_T{:d}_fusion_epoc.txt'.format(TRIAL), sep=' ')\n",
        "            fiveSecondWindowStamps = pd.read_csv(dataPath + \"S1_T{:d}_TimestampEvery5Seconds.txt\".format(TRIAL)\n",
        "                                                 , sep=' ').values\n",
        "\n",
        "            #Get Normalizing Values\n",
        "            shimmerData = shimmerFile['PPG'].values\n",
        "            epocData = epocFile[['AF3', 'F7', 'F3', 'FC5', 'T7',\n",
        "                                 'P7', 'O1', 'O2', 'P8', 'T8',\n",
        "                                 'FC6', 'F4', 'F8', 'AF4']].values\n",
        "\n",
        "            epocMean, epocStd = epocData.mean(), epocData.std()\n",
        "            shimmerMean, shimmerStd = shimmerData.mean(), shimmerData.std()\n",
        "\n",
        "            #Create and fill Data Container\n",
        "            container = DataContainer()\n",
        "\n",
        "            for i in range(fiveSecondWindowStamps.shape[0] - 1):\n",
        "                container.createWindow(fiveSecondWindowStamps[i],\n",
        "                                       fiveSecondWindowStamps[i+1],\n",
        "                                       1000, 1000,\n",
        "                                       shimmerScaling=(shimmerMean, shimmerStd),\n",
        "                                       epocScaling=(epocMean,epocStd))\n",
        "\n",
        "            container.fillData(shimmerData=shimmerFile, epocData = epocFile)\n",
        "            container.normalizeData()\n",
        "            container.createMetrics()\n",
        "            container.dumpWindowsToFiles(destinationPath, TRIAL)\n",
        "\n",
        "            print(\"Finish Creating Pickle Files from trial {:d}\".format(TRIAL))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The current working directory is /content/RunTimeDir\n",
            "\n",
            "Final preprocessing of D1\n",
            "Creation of the directory ./Data/D1/final/ failed\n",
            "Finish Creating Pickle Files from trial 3\n",
            "Finish Creating Pickle Files from trial 4\n",
            "Finish Creating Pickle Files from trial 5\n",
            "Finish Creating Pickle Files from trial 6\n",
            "Finish Creating Pickle Files from trial 7\n",
            "Final preprocessing of D2\n",
            "Creation of the directory ./Data/D2/final/ failed\n",
            "Finish Creating Pickle Files from trial 2\n",
            "Finish Creating Pickle Files from trial 4\n",
            "Finish Creating Pickle Files from trial 5\n",
            "Finish Creating Pickle Files from trial 6\n",
            "Finish Creating Pickle Files from trial 7\n",
            "Final preprocessing of D3\n",
            "Creation of the directory ./Data/D3/final/ failed\n",
            "Finish Creating Pickle Files from trial 3\n",
            "Finish Creating Pickle Files from trial 4\n",
            "Finish Creating Pickle Files from trial 5\n",
            "Finish Creating Pickle Files from trial 6\n",
            "Finish Creating Pickle Files from trial 7\n",
            "Final preprocessing of D4\n",
            "Creation of the directory ./Data/D4/final/ failed\n",
            "Finish Creating Pickle Files from trial 1\n",
            "Finish Creating Pickle Files from trial 2\n",
            "Finish Creating Pickle Files from trial 3\n",
            "Finish Creating Pickle Files from trial 4\n",
            "Finish Creating Pickle Files from trial 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hk5Npfa2Zsy",
        "colab_type": "text"
      },
      "source": [
        "#Test GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQTUePAy2dN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import torch\n",
        "# import pycuda.driver as cuda\n",
        "\n",
        "\n",
        "# print(\"Do we have GPUs available?\")\n",
        "# print(torch.cuda.is_available())\n",
        "\n",
        "# ## Get Id of default device\n",
        "# cuda.init()\n",
        "# print(\"Default device: {:d}\".format(torch.cuda.current_device()) )\n",
        "# print(\"GPU name: {:s}\".format(cuda.Device(0).name()))\n",
        "\n",
        "# print(\"GPU name with Pytorch: {:s}\".format(torch.cuda.get_device_name(0) ))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z1BL5rWxCWa",
        "colab_type": "text"
      },
      "source": [
        "#Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nb-tgMZ1ZPg",
        "colab_type": "text"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilPowRJBxWIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ConvNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, out_1=2, out_2=1, out_3=12, activation=\"relu\"):\n",
        "\n",
        "        super(ConvNetwork,self).__init__()\n",
        "\n",
        "        #SET activation\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'selu':\n",
        "            self.activation = nn.SELU()\n",
        "        elif activation == 'elu':\n",
        "            self.activation = nn.ELU()\n",
        "\n",
        "        #First Convolutional Layer\n",
        "        self.cnn1 = nn.Conv2d(in_channels=14,out_channels=out_1,kernel_size=1,padding=0)\n",
        "        #self.maxpool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(out_1)\n",
        "        self.relu1 = self.activation\n",
        "\n",
        "        #Second Convolutional Layer\n",
        "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=3, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(out_2)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu2 = self.activation\n",
        "\n",
        "        # Third Convolutional Layer\n",
        "        self.cnn3 = nn.Conv2d(in_channels=out_2, out_channels=out_3, kernel_size=1, padding=0)\n",
        "        self.bn3 = nn.BatchNorm2d(out_3)\n",
        "        self.relu3 = self.activation\n",
        "\n",
        "        #Fully connected Layer\n",
        "        self.fc1 = nn.Linear(out_3*26*4, 784)\n",
        "        self.fc_bn = nn.BatchNorm1d(784)\n",
        "        self.fc2 = nn.Linear(784, 2)\n",
        "        self.relu4 = self.activation\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        out = self.cnn1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        #out = self.maxpool1(out)\n",
        "        out = self.cnn2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.maxpool2(out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        out = self.cnn3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu3(out)\n",
        "\n",
        "        out = out.view(out.size(0),-1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc_bn(out)\n",
        "        out = self.relu4(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHUvV76u1d3T",
        "colab_type": "text"
      },
      "source": [
        "##EEG dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPooMyR_1hug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from os.path import isfile, join\n",
        "from os import listdir\n",
        "import pickle\n",
        "\n",
        "\n",
        "class EegDataset(Dataset):\n",
        "\n",
        "    def __init__(self,datasetPaths=[\"./Dataset/\"],transform =None):\n",
        "\n",
        "        self.datasetPaths = datasetPaths\n",
        "        self.files = []\n",
        "        self.len = 0\n",
        "        \n",
        "        for datasetPath in self.datasetPaths:\n",
        "            tempArr = [join(datasetPath, f) for f in listdir(datasetPath) if isfile(join(datasetPath, f))]\n",
        "            self.files += tempArr\n",
        "            self.len += len(tempArr)\n",
        "\n",
        "        self.x = torch.zeros((self.len, 14, 51, 7))\n",
        "        self.y = torch.zeros(len(self.files),dtype=torch.long)\n",
        "\n",
        "        self.positiveLength = 0\n",
        "        self.negativeLength = 0\n",
        "        self.positiveIdx = []\n",
        "        self.negativeIdx = []\n",
        "\n",
        "        for i in range(len(self.files)):\n",
        "\n",
        "            with open(self.files[i],'rb') as f1:\n",
        "                dataDict = pickle.load(f1)\n",
        "                self.x[i, :, :, :] = torch.torch.from_numpy(dataDict['data'])\n",
        "                self.y[i] = int(dataDict['label'])\n",
        "\n",
        "                if self.y[i] == 1:\n",
        "                    self.positiveLength += 1\n",
        "                    self.positiveIdx.append(i)\n",
        "                elif self.y[i] == 0:\n",
        "                    self.negativeLength += 1\n",
        "                    self.negativeIdx.append(i)\n",
        "\n",
        "\n",
        "        # Normalizin Values\n",
        "        self.xMean = self.x.mean()\n",
        "        self.xStd  = self.x.std()\n",
        "        self.arbitraryScaling = self.x.max()*3/8\n",
        "        self.xMax = self.x.max()\n",
        "        self.xMin = self.x.min()\n",
        "\n",
        "        # Scaling by (max - min)\n",
        "        self.x = self.x / (self.xMax - self.xMin + 1e-7)\n",
        "\n",
        "        #Scaling by mean and std\n",
        "        #self.x = (self.x - self.xMean) / (self.xStd + 1e-7)\n",
        "\n",
        "        #Scaling with arbitrary scaling factor\n",
        "        # self.x = self.x / (self.arbitraryScaling)\n",
        "\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        sample = self.x[index], self.y[index]\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV5YSRxz1ibC",
        "colab_type": "text"
      },
      "source": [
        "##Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXmo6mox1kzt",
        "colab_type": "code",
        "outputId": "c4c44c54-7508-4178-8d14-06628df174ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.optim as optim\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "def save_checkpoint(optimizer, model, epoch, trLossH, valLossH, valAccH, filename):\n",
        "    checkpoint_dict = {'optimizer': optimizer.state_dict(),\n",
        "                       'model': model.state_dict(),\n",
        "                       'epoch': epoch,\n",
        "                       'trLossH': trLossH,\n",
        "                       'valLossH': valLossH,\n",
        "                       'valAccH': valAccH}\n",
        "\n",
        "    torch.save(checkpoint_dict, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(optimizer, model, filename):\n",
        "    checkpoint_dict = torch.load(filename, map_location=DEVICE)\n",
        "    epoch = checkpoint_dict['epoch']\n",
        "    trLossH = checkpoint_dict['trLossH']\n",
        "    valLossH = checkpoint_dict['valLossH']\n",
        "    valAccH = checkpoint_dict['valAccH']\n",
        "\n",
        "    model.load_state_dict(checkpoint_dict['model'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "\n",
        "    return epoch, trLossH, valLossH, valAccH\n",
        "\n",
        "def trainModel(model, optimizer, lossFunction, trainLoader, validLoader, fold, modelNumb, verbose=False ):\n",
        "    #TrainLogger\n",
        "    valAccuracyHistory = []\n",
        "    valLossHistory = []\n",
        "    trLossHistory = []\n",
        "\n",
        "    #Train Fold\n",
        "    #Reset Max accuracy\n",
        "    maxValidationAccuracy = 0.7\n",
        "\n",
        "    for epoch in range(totalEpochs):\n",
        "\n",
        "        #Train Model with training set\n",
        "        model.train()\n",
        "        for x, y in trainLoader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)  \n",
        "            optimizer.zero_grad()\n",
        "            yHat = model(x)\n",
        "            loss = lossFunction(yHat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            trLossHistory.append(loss.item())\n",
        "\n",
        "\n",
        "        #Calculate Accuracy with validation set\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, y in validLoader:\n",
        "                x, y = x.to(DEVICE), y.to(DEVICE)  \n",
        "                z = model(x)\n",
        "                yHat = z.argmax(1)\n",
        "                correct += (yHat == y).sum().item()\n",
        "                total += y.shape[0]\n",
        "\n",
        "        accuracy = correct / total\n",
        "        valAccuracyHistory.append(accuracy)\n",
        "\n",
        "        #Print Accuracy\n",
        "        if epoch%20 == 0 and epoch != 0 and verbose:\n",
        "            #print(\"Loss in Training in epoch {:d} set:       {:.5f}\".format(epoch, loss))\n",
        "            print(\"Max validation accuracy in epoch {:d} set: {:.5f}\".format(epoch, max(valAccuracyHistory)))\n",
        "\n",
        "        # Save Check Point - Save only the best per fold\n",
        "        if accuracy > maxValidationAccuracy:\n",
        "            maxValidationAccuracy = accuracy\n",
        "            \n",
        "            checkpoint_filename = MODELS_DIRECTORY + \\\n",
        "                                  'Model{:d}/classifier-f{:02d}.pkl'.format(modelNumb, fold)\n",
        "            \n",
        "            save_checkpoint(optimizer, model, epoch, trLossHistory, valLossHistory, valAccuracyHistory,\n",
        "                            checkpoint_filename)\n",
        "\n",
        "            checkpoint_txt = MODELS_DIRECTORY + \\\n",
        "                            'Model{:d}_History/classifier-f{:02d}-e{:03d}-acc{:0.5f}.txt'.format(\n",
        "                             modelNumb, fold, epoch, maxValidationAccuracy)\n",
        "                \n",
        "            with open(checkpoint_txt,'w') as f:\n",
        "                pass\n",
        "\n",
        "            maxValidationAccuracy = accuracy\n",
        "            \n",
        "    return valAccuracyHistory\n",
        "\n",
        "def fiveKFoldTraining(trainData, folds, cnn, totalEpochs, modelNumb):\n",
        "\n",
        "    valAccuracyPlots = []\n",
        "    loss = 0\n",
        "    for i in range(5):\n",
        "        print(\"Start Training Fold {:d}\".format(i))\n",
        "        #Create Data loaders\n",
        "        trainLoader = DataLoader(trainData, batch_size=16, sampler=folds[i]['train'])\n",
        "        validLoader = DataLoader(trainData, batch_size=16, sampler=folds[i]['val'])\n",
        "\n",
        "        #Create the model, optimizer and loss function\n",
        "        torch.manual_seed(RANDOM_SEED)\n",
        "        model = ConvNetwork( out_1=cnn['CONV_LAYER1'],\n",
        "                             out_2=cnn['CONV_LAYER2'],\n",
        "                             out_3=cnn['CONV_LAYER3'],\n",
        "                             activation=cnn['ACTIVATION'])\n",
        "        \n",
        "        model.to(DEVICE)\n",
        "        lossFunction = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(params=model.parameters(), lr=0.01, weight_decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "        #Train\n",
        "        valAccuracyHistory = trainModel(model, \n",
        "                                        optimizer, \n",
        "                                        lossFunction, \n",
        "                                        trainLoader, \n",
        "                                        validLoader, \n",
        "                                        i, modelNumb)\n",
        "        \n",
        "        maxValAccPerFold =  max(valAccuracyHistory)\n",
        "        valAccuracyPlots.append(valAccuracyHistory)\n",
        "        print(\"Max Validation Accuracy fold {:02d}:        {:0.5f}\\n\".format(i, maxValAccPerFold))\n",
        "\n",
        "\n",
        "    #Save data Accuracies\n",
        "    statsFile = MODELS_DIRECTORY + 'Model{:d}/foldAccuracies.pkl'.format(modelNumb)\n",
        "    with open(statsFile, 'wb') as f:\n",
        "        pickle.dump(valAccuracyPlots, f)\n",
        "\n",
        "    #Print training summary\n",
        "    for i in range(5):\n",
        "        print(\"Max accuracy in fold {:d}: {:.4f}\".format(i,max(valAccuracyPlots[i])))\n",
        "\n",
        "    averageMaxAcc = sum([max(valAccuracyPlots[i]) for i in range(5)])/5\n",
        "    print(\"Average Max Accuracy: {:.5f}\".format(averageMaxAcc))\n",
        "\n",
        "\n",
        "def createModelDirectory(modelNumber, checkpointDirectory=True):\n",
        "    if checkpointDirectory:\n",
        "        path = MODELS_DIRECTORY + \"Model{:02d}\".format(modelNumber)\n",
        "    else:\n",
        "        path = MODELS_DIRECTORY + \"Model{:02d}_History\".format(modelNumber)\n",
        "        \n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError:\n",
        "        print(\"Creation of the directory %s failed\" % path)\n",
        "    else:\n",
        "        print(\"Successfully created the directory %s \" % path)\n",
        "\n",
        "        \n",
        "def get5Folds(trainDataSize):\n",
        "\n",
        "    #Shuffle training data\n",
        "    split = int(0.2 * trainDataSize)\n",
        "    np.random.seed(seed=RANDOM_SEED)\n",
        "    idxArr = np.random.permutation(trainDataSize)\n",
        "\n",
        "    #Create 5 folds for cross Validation\n",
        "    folds = []\n",
        "    total = trainDataSize\n",
        "\n",
        "    for i in range(5):\n",
        "        valRange = range(split*i, split*(i+1) if split*(i+1) < total else total)\n",
        "        valIdx = idxArr[valRange]\n",
        "        trainIdx = np.delete(idxArr, valRange)\n",
        "        trainSampler = SubsetRandomSampler(trainIdx)\n",
        "        valSampler = SubsetRandomSampler(valIdx)\n",
        "        folds.append({'train': trainSampler, 'val': valSampler})\n",
        "\n",
        "    return folds\n",
        "\n",
        "#Cnn Architectures -- Best Model cnn8!!\n",
        "cnn1 = {\"CONV_LAYER1\": 32, \"CONV_LAYER2\": 64, \"CONV_LAYER3\": 12, \"ACTIVATION\": 'selu'}\n",
        "cnn2 = {\"CONV_LAYER1\": 32, \"CONV_LAYER2\": 64, \"CONV_LAYER3\": 12, \"ACTIVATION\": 'elu'}\n",
        "cnn3 = {\"CONV_LAYER1\": 32, \"CONV_LAYER2\": 64, \"CONV_LAYER3\": 12, \"ACTIVATION\": 'relu'}\n",
        "cnn4 = {\"CONV_LAYER1\": 16, \"CONV_LAYER2\": 32, \"CONV_LAYER3\": 6, \"ACTIVATION\": 'selu'}\n",
        "cnn5 = {\"CONV_LAYER1\": 16, \"CONV_LAYER2\": 32, \"CONV_LAYER3\": 6, \"ACTIVATION\": 'elu'}\n",
        "cnn6 = {\"CONV_LAYER1\": 16, \"CONV_LAYER2\": 32, \"CONV_LAYER3\": 6, \"ACTIVATION\": 'relu'}\n",
        "cnn7 = {\"CONV_LAYER1\": 64, \"CONV_LAYER2\": 128, \"CONV_LAYER3\": 24, \"ACTIVATION\": 'selu'}\n",
        "cnn8 = {\"CONV_LAYER1\": 64, \"CONV_LAYER2\": 128, \"CONV_LAYER3\": 24, \"ACTIVATION\": 'elu'}\n",
        "cnn9 = {\"CONV_LAYER1\": 64, \"CONV_LAYER2\": 128, \"CONV_LAYER3\": 24, \"ACTIVATION\": 'relu'}\n",
        "\n",
        "\n",
        "cnn10 = {\"CONV_LAYER1\": 128, \"CONV_LAYER2\": 256, \"CONV_LAYER3\": 48, \"ACTIVATION\": 'elu'}\n",
        "\n",
        "#Global variables\n",
        "DEVICE = 'cuda'\n",
        "MODELS_DIRECTORY = './Models/'\n",
        "\n",
        "#RandomSeed\n",
        "#Model 2 and Model 3\n",
        "#RANDOM_SEED = 742\n",
        "#Model 4\n",
        "RANDOM_SEED = 480\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    #Create Training data object\n",
        "    paths1 = [\"./Data/D1/final\", \"./Data/D2/final\", \"./Data/D3/final\"]\n",
        "    paths2 = [\"./Data/D2/final\", \"./Data/D3/final\", \"./Data/D4/final\"]\n",
        "    paths3 = [\"./Data/D1/final\", \"./Data/D2/final\", \"./Data/D4/final\"]\n",
        "    paths4 = [\"./Data/D1/final\", \"./Data/D3/final\", \"./Data/D4/final\"]\n",
        "    paths = [paths1,paths2,paths3,paths4]\n",
        "    \n",
        "    #trainData = EegDataset(datasetPaths=paths1)\n",
        "\n",
        "    #Train Models\n",
        "    totalEpochs = 200\n",
        "    print(\"Start training\")\n",
        "    #cnnArch = [cnn1,cnn2,cnn3,cnn4,cnn5,cnn6,cnn7,cnn8,cnn9]\n",
        "    cnnArch = [cnn8]\n",
        "    #cnnArch = [cnn10]\n",
        "    modelNumber = 70\n",
        "\n",
        "#   #Training Loop\n",
        "#     for path in paths:  \n",
        "#         #Load Data\n",
        "#         trainData = EegDataset(datasetPaths=path)\n",
        "#         folds = get5Folds(len(trainData))\n",
        "#         #Print Information\n",
        "#         print(\"Start Training Model {:d}\".format(modelNumber))\n",
        "#         print(\"Number of negative examples \",trainData.negativeLength)\n",
        "#         print(\"Number of positive examples \", trainData.positiveLength)\n",
        "#         print(\"Dataset size \", len(trainData))\n",
        "#         #Create Directories\n",
        "#         createModelDirectory(modelNumber, checkpointDirectory=True)\n",
        "#         createModelDirectory(modelNumber, checkpointDirectory=False)\n",
        "#         #Train Model\n",
        "#         fiveKFoldTraining(trainData, folds, cnn8, totalEpochs, modelNumber)\n",
        "#         modelNumber += 1\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training\n",
            "Start Training Model 70\n",
            "Number of negative examples  559\n",
            "Number of positive examples  526\n",
            "Dataset size  1085\n",
            "Successfully created the directory ./Models/Model70 \n",
            "Successfully created the directory ./Models/Model70_History \n",
            "Start Training Fold 0\n",
            "Max Validation Accuracy fold 00:        0.80184\n",
            "\n",
            "Start Training Fold 1\n",
            "Max Validation Accuracy fold 01:        0.78802\n",
            "\n",
            "Start Training Fold 2\n",
            "Max Validation Accuracy fold 02:        0.76037\n",
            "\n",
            "Start Training Fold 3\n",
            "Max Validation Accuracy fold 03:        0.75576\n",
            "\n",
            "Start Training Fold 4\n",
            "Max Validation Accuracy fold 04:        0.75576\n",
            "\n",
            "Max accuracy in fold 0: 0.8018\n",
            "Max accuracy in fold 1: 0.7880\n",
            "Max accuracy in fold 2: 0.7604\n",
            "Max accuracy in fold 3: 0.7558\n",
            "Max accuracy in fold 4: 0.7558\n",
            "Average Max Accuracy: 0.77235\n",
            "Start Training Model 71\n",
            "Number of negative examples  616\n",
            "Number of positive examples  552\n",
            "Dataset size  1168\n",
            "Successfully created the directory ./Models/Model71 \n",
            "Successfully created the directory ./Models/Model71_History \n",
            "Start Training Fold 0\n",
            "Max Validation Accuracy fold 00:        0.74678\n",
            "\n",
            "Start Training Fold 1\n",
            "Max Validation Accuracy fold 01:        0.80258\n",
            "\n",
            "Start Training Fold 2\n",
            "Max Validation Accuracy fold 02:        0.79828\n",
            "\n",
            "Start Training Fold 3\n",
            "Max Validation Accuracy fold 03:        0.81545\n",
            "\n",
            "Start Training Fold 4\n",
            "Max Validation Accuracy fold 04:        0.74678\n",
            "\n",
            "Max accuracy in fold 0: 0.7468\n",
            "Max accuracy in fold 1: 0.8026\n",
            "Max accuracy in fold 2: 0.7983\n",
            "Max accuracy in fold 3: 0.8155\n",
            "Max accuracy in fold 4: 0.7468\n",
            "Average Max Accuracy: 0.78197\n",
            "Start Training Model 72\n",
            "Number of negative examples  558\n",
            "Number of positive examples  534\n",
            "Dataset size  1092\n",
            "Successfully created the directory ./Models/Model72 \n",
            "Successfully created the directory ./Models/Model72_History \n",
            "Start Training Fold 0\n",
            "Max Validation Accuracy fold 00:        0.75229\n",
            "\n",
            "Start Training Fold 1\n",
            "Max Validation Accuracy fold 01:        0.80734\n",
            "\n",
            "Start Training Fold 2\n",
            "Max Validation Accuracy fold 02:        0.79358\n",
            "\n",
            "Start Training Fold 3\n",
            "Max Validation Accuracy fold 03:        0.82110\n",
            "\n",
            "Start Training Fold 4\n",
            "Max Validation Accuracy fold 04:        0.81651\n",
            "\n",
            "Max accuracy in fold 0: 0.7523\n",
            "Max accuracy in fold 1: 0.8073\n",
            "Max accuracy in fold 2: 0.7936\n",
            "Max accuracy in fold 3: 0.8211\n",
            "Max accuracy in fold 4: 0.8165\n",
            "Average Max Accuracy: 0.79817\n",
            "Start Training Model 73\n",
            "Number of negative examples  580\n",
            "Number of positive examples  536\n",
            "Dataset size  1116\n",
            "Successfully created the directory ./Models/Model73 \n",
            "Successfully created the directory ./Models/Model73_History \n",
            "Start Training Fold 0\n",
            "Max Validation Accuracy fold 00:        0.81614\n",
            "\n",
            "Start Training Fold 1\n",
            "Max Validation Accuracy fold 01:        0.77578\n",
            "\n",
            "Start Training Fold 2\n",
            "Max Validation Accuracy fold 02:        0.73543\n",
            "\n",
            "Start Training Fold 3\n",
            "Max Validation Accuracy fold 03:        0.81614\n",
            "\n",
            "Start Training Fold 4\n",
            "Max Validation Accuracy fold 04:        0.72197\n",
            "\n",
            "Max accuracy in fold 0: 0.8161\n",
            "Max accuracy in fold 1: 0.7758\n",
            "Max accuracy in fold 2: 0.7354\n",
            "Max accuracy in fold 3: 0.8161\n",
            "Max accuracy in fold 4: 0.7220\n",
            "Average Max Accuracy: 0.77309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqOGLYUtjWwz",
        "colab_type": "text"
      },
      "source": [
        "#Copy Models to Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0MDmu6C9IeL",
        "colab_type": "code",
        "outputId": "cf9e82fc-05c4-40cd-9603-5eacc8619a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "dstPath = \"./gdrive/My Drive/ToshibaJuanAntonio/\"+\\\n",
        "       \"Trabajos Purdue/Surgical Intuitive Grant/\"+\\\n",
        "       \"MentalWorkloadPrediction/Models/\"\n",
        "\n",
        "dstPath = dstPath+\"Trial4/\"\n",
        "srcPath = \"/content/RunTimeDir/Models/\"\n",
        "\n",
        "\n",
        "try:\n",
        "    os.makedirs(dstPath)\n",
        "except OSError:\n",
        "    print(\"Creation of the directory %s failed\" % dstPath)\n",
        "else:\n",
        "    print(\"Successfully created the directory %s \" % dstPath)\n",
        "\n",
        "#Copy models folder \n",
        "from distutils.dir_util import copy_tree\n",
        "copy_tree(srcPath, dstPath)\n",
        "\n",
        "\n",
        "os.chdir('/content/RunTimeDir')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully created the directory ./gdrive/My Drive/ToshibaJuanAntonio/Trabajos Purdue/Surgical Intuitive Grant/MentalWorkloadPrediction/Models/Trial4/ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0N0s6APmXyQ",
        "colab_type": "text"
      },
      "source": [
        "#Load Models to Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOwhWckYmaqO",
        "colab_type": "code",
        "outputId": "8a2baea1-db92-486b-84fc-e5139c61f372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def copyWholeDirectory(srcPath,dstPath):\n",
        "    directoriesToCopy = os.listdir(srcPath)\n",
        "\n",
        "    for p in directoriesToCopy:\n",
        "        if os.path.isdir(srcPath+p):\n",
        "            copyWholeDirectory(srcPath+p+'/',dstPath+p+'/')\n",
        "        else:\n",
        "            try:\n",
        "                os.makedirs(dstPath)\n",
        "            except OSError:\n",
        "                pass\n",
        "                #print(\"Creation of the directory %s failed\" % dstPath)\n",
        "            else:\n",
        "                pass\n",
        "                #print(\"Successfully created the directory %s \" % dstPath)\n",
        "\n",
        "            shutil.copy(srcPath+p, dstPath)\n",
        "\n",
        "            \n",
        "os.chdir('/content')\n",
        "\n",
        "#Remove Previous ModelsDirectory\n",
        "try:\n",
        "    shutil.rmtree('./RunTimeDir/Models')\n",
        "except:\n",
        "    print(\"Directory not found\")\n",
        "else:\n",
        "    print(\"Deleting previous Directory\")\n",
        "finally:\n",
        "    print()\n",
        "\n",
        "srcPath = \"./gdrive/My Drive/ToshibaJuanAntonio/\"+\\\n",
        "       \"Trabajos Purdue/Surgical Intuitive Grant/\"+\\\n",
        "       \"MentalWorkloadPrediction/Models/\"\n",
        "\n",
        "srcPath = srcPath+\"Trial2/\"\n",
        "\n",
        "dstPath = \"./RunTimeDir/Models/\"\n",
        "\n",
        "\n",
        "try:\n",
        "    os.makedirs(dstPath)\n",
        "except OSError:\n",
        "    print(\"Creation of the directory %s failed\" % dstPath)\n",
        "else:\n",
        "    print(\"Successfully created the directory %s \" % dstPath)\n",
        "\n",
        "#Copy models folder \n",
        "print(\"Copy models from drive\")\n",
        "copyWholeDirectory(srcPath, dstPath)\n",
        "\n",
        "\n",
        "os.chdir('/content/RunTimeDir')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleting previous Directory\n",
            "\n",
            "Successfully created the directory ./RunTimeDir/Models/ \n",
            "Copy models from drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rU-qHoLJDmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# os.chdir('/content')\n",
        "# src='./gdrive/My Drive/ToshibaJuanAntonio/Trabajos Purdue/Surgical Intuitive Grant/MentalWorkloadPrediction/Models/Trial1/Model40/classifier-f03.pkl'\n",
        "\n",
        "# dst='./RunTimeDir/Models/Model40/'\n",
        "# os.mkdir(dst)\n",
        "# shutil.copy(src, dst)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yJ-MYfKNMwp",
        "colab_type": "text"
      },
      "source": [
        "#Testing Module\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_nAsokINP1B",
        "colab_type": "code",
        "outputId": "d2199094-2c13-43bb-b3eb-4192334447a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_checkpoint(optimizer, model, filename):\n",
        "    checkpoint_dict = torch.load(filename, map_location=DEVICE)\n",
        "    epoch = checkpoint_dict['epoch']\n",
        "    trLossH = checkpoint_dict['trLossH']\n",
        "    valLossH = checkpoint_dict['valLossH']\n",
        "    valAccH = checkpoint_dict['valAccH']\n",
        "\n",
        "    model.load_state_dict(checkpoint_dict['model'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "\n",
        "    return epoch, trLossH, valLossH, valAccH\n",
        "\n",
        "\n",
        "def calculateAcc(model, loader):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, y in loader:\n",
        "                x, y = x.to(DEVICE), y.to(DEVICE)  \n",
        "                z = model(x)\n",
        "                yHat = z.argmax(1)\n",
        "                correct += (yHat == y).sum().item()\n",
        "                total += y.shape[0]\n",
        "\n",
        "        accuracy = correct / total\n",
        "        return accuracy\n",
        "\n",
        "#Global Variables\n",
        "\n",
        "cnn8 = {\"CONV_LAYER1\": 64, \"CONV_LAYER2\": 128, \"CONV_LAYER3\": 24, \"ACTIVATION\": 'elu'}\n",
        "DEVICE ='cuda'\n",
        "RANDOM_SEED = 480\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    print(\"Test Model on test set\")\n",
        "    \n",
        "    modelNumber = 50\n",
        "    pathToClassifiers = \"./Models/Model{:02d}/\".format(modelNumber)\n",
        "    pathToTestData  = \"./Data/D4/final\"\n",
        "    pathToTrainData = [\"./Data/D1/final\", \"./Data/D2/final\", \"./Data/D3/final\"]\n",
        "    accDict={'train':[], 'val':[], 'test':[]}\n",
        "    \n",
        "    #Load test Dataset\n",
        "    testData = EegDataset(datasetPaths=[pathToTestData])\n",
        "    testLoader = DataLoader(trainData, batch_size=100, shuffle=True )\n",
        "    print(\"Test Negative: \", testData.negativeLength)\n",
        "    print(\"Test Positive: \", testData.positiveLength)\n",
        "    print(\"Test set Accuracy:\")\n",
        "    \n",
        "    #Load train dataset\n",
        "    trainData = EegDataset(datasetPaths=path)\n",
        "    folds = get5Folds(len(trainData))\n",
        "    \n",
        "    for fold in range(5):\n",
        "        #Get train and valid loaders\n",
        "        trainLoader = DataLoader(trainData, batch_size=16, sampler=folds[i]['train'])\n",
        "        validLoader = DataLoader(trainData, batch_size=16, sampler=folds[i]['val'])\n",
        "        \n",
        "        #Load Model\n",
        "        torch.manual_seed(RANDOM_SEED)\n",
        "        model = ConvNetwork( out_1=cnn8['CONV_LAYER1'],\n",
        "                             out_2=cnn8['CONV_LAYER2'],\n",
        "                             out_3=cnn8['CONV_LAYER3'],\n",
        "                             activation=cnn8['ACTIVATION'])\n",
        "\n",
        "        optimizer = optim.SGD(params=model.parameters(), \n",
        "                              lr=0.01, \n",
        "                              weight_decay=1e-6, \n",
        "                              momentum=0.9, \n",
        "                              nesterov=True)\n",
        "\n",
        "        load_checkpoint(optimizer, model, pathToClassifiers+'classifier-f{:02d}.pkl'.format(fold))\n",
        "        model.to(DEVICE)\n",
        "\n",
        "        #Calculate Accuracies\n",
        "        accuracyVal = calculateAcc(model,testLoader)\n",
        "        accDict['test'] += [accuracyVal]\n",
        "        print(\"\\tfold{:02d}: {:0.5f}\".format(fold, accuracyVal))\n",
        "        \n",
        "   \n",
        "    print(\"Average Accuracy: {:0.5f}\".format(np.mean(accDict['test'])))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Model on test set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2b6bb8c41c5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m#Load test Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtestData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEegDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetPaths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpathToTestData\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mtestLoader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Negative: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegativeLength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EegDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FtYNbV9eRC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}