{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEG_Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jabarragann/PytorchTutorialsJuanAntonio/blob/master/EEG_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH0rzpJvZdj8",
        "colab_type": "text"
      },
      "source": [
        "# Connecting to Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrzQqrG9YtTW",
        "colab_type": "code",
        "outputId": "2c0e8eee-ad51-47b9-8d72-cf0f5970ba9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngYj70w1ZOOE",
        "colab_type": "code",
        "outputId": "486c79f0-1134-48d8-8e5e-f596866b7278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "# !ls\n",
        "# !mkdir -p 'gdrive/My Drive/ToshibaJuanAntonio/Trabajos Purdue/Surgical Intuitive Grant/MentalWorkloadPrediction/Data/D4/'\n",
        "# !mkdir -p 'gdrive/My Drive/ToshibaJuanAntonio/Trabajos Purdue/Surgical Intuitive Grant/MentalWorkloadPrediction/Data/D4/preprocess'\n",
        "!ls\n",
        "import os\n",
        "os.chdir('/content')\n",
        "!ls \n",
        "\n",
        "!pip install pycuda"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data\n",
            "gdrive\tRunTimeDir  sample_data\n",
            "Collecting pycuda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/29/5a3eb66c2f1a4adc681f6c8131e9ed677af31b0c8a78726d540bd44b3403/pycuda-2019.1.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 35.3MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2 (from pycuda)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/96/00416762a3eda8876a17d007df4a946f46b2e4ee1057e0b9714926472ef8/pytools-2019.1.1.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 26.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest>=2 in /usr/local/lib/python3.6/dist-packages (from pycuda) (3.6.4)\n",
            "Requirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.0)\n",
            "Collecting appdirs>=1.4.0 (from pycuda)\n",
            "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
            "Collecting mako (from pycuda)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/29/8016763284d8fab844224f7cc5675cb4a388ebda0eb5a403260187e48435/Mako-1.0.13.tar.gz (460kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 50.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.16.4)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (19.1.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (41.0.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (1.8.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (7.0.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (0.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Building wheels for collected packages: pycuda, pytools, mako\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/d5/351a6b47b20d417e82a669cf53f8cb4d7b55a57f73cbd05184\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/df/0b/75ac4572aaa93e3eba6a58472635d0fda907f5f4cf884a3a0c\n",
            "  Building wheel for mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/0e/59/0e7f24103d1ebce045037aa17b75548a8387f5e7d2d0011fdc\n",
            "Successfully built pycuda pytools mako\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.3 mako-1.0.13 pycuda-2019.1 pytools-2019.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9QDqr3Iw93H",
        "colab_type": "text"
      },
      "source": [
        "#Setting up the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osiCHsR22VSf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "1. Delete current run time directory\n",
        "2. Create all needed directories\n",
        "3. Copy data from google drive account to Colab directory\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rM_oxsX0CHc",
        "colab_type": "code",
        "outputId": "ec9076ed-0d65-47d4-f1fe-68d29415b031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def createDirectories(rootPath, *args):\n",
        "  \n",
        "  print(\"Creating directories\")\n",
        "  for idx, path in enumerate(args):\n",
        "    try:  \n",
        "      os.makedirs(rootPath + path)\n",
        "    except OSError:  \n",
        "      print (\"{:d}: Creation of the directory {} failed\".format(idx,path))\n",
        "    else:  \n",
        "      print (\"{:d}: Successfully created the directory {}\".format(idx, path))\n",
        "  print()\n",
        "\n",
        "def copyFiles(srcRootPath,dstRootPath, *directories):\n",
        "  print(\"copy data from google drive\")\n",
        "  for idx, path in enumerate(directories):\n",
        "    print(\"{:d}: Copy files from: {:s}\".format(idx,path) )\n",
        "    completePath = srcRootPath+path\n",
        "    srcFiles = os.listdir(completePath)\n",
        "    srcFiles.sort()\n",
        "\n",
        "    for file in srcFiles:\n",
        "        srcPath = completePath + file\n",
        "        dstPath = dstRootPath+path\n",
        "\n",
        "        if os.path.isfile(srcPath):\n",
        "            shutil.copy(srcPath, dstPath)\n",
        "            \n",
        "  print()\n",
        "\n",
        "#Set /content as working directory\n",
        "os.chdir('/content')\n",
        "\n",
        "#Remove Previous RuntimeDirectory\n",
        "try:\n",
        "  shutil.rmtree('./RunTimeDir')\n",
        "except:\n",
        "  print(\"RunTimeDirectory not found\")\n",
        "else:\n",
        "  print(\"Deleting previous RunTimeDirectory\")\n",
        "finally:\n",
        "  print()\n",
        "\n",
        "\n",
        "# Create new directories\n",
        "rootPath = \"./RunTimeDir/Data/\"\n",
        "directories = ['D1/preprocessed','D1/final',\n",
        "               'D2/preprocessed','D2/final',\n",
        "               'D3/preprocessed','D3/final',\n",
        "               'D4/preprocessed','D4/final']\n",
        "\n",
        "createDirectories(rootPath, *directories)\n",
        "\n",
        "\n",
        "#Copy data from google drive to RuntimeDir\n",
        "\n",
        "gdrivePath ='gdrive/My Drive/ToshibaJuanAntonio/Trabajos Purdue/Surgical Intuitive Grant/MentalWorkloadPrediction/Data/'\n",
        "srcDirectories = ['D1/preprocessed/']\n",
        "dstRootPath = \"./RunTimeDir/Data/\"\n",
        "\n",
        "copyFiles(gdrivePath, dstRootPath, *srcDirectories)\n",
        "\n",
        "\n",
        "# Check current working directory\n",
        "os.chdir('/content/RunTimeDir')\n",
        "path = os.getcwd()  \n",
        "print (\"The current working directory is %s\" % path, end='\\n\\n') \n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RunTimeDirectory not found\n",
            "\n",
            "Creating directories\n",
            "0: Successfully created the directory D1/preprocessed\n",
            "1: Successfully created the directory D1/final\n",
            "2: Successfully created the directory D2/preprocessed\n",
            "3: Successfully created the directory D2/final\n",
            "4: Successfully created the directory D3/preprocessed\n",
            "5: Successfully created the directory D3/final\n",
            "6: Successfully created the directory D4/preprocessed\n",
            "7: Successfully created the directory D4/final\n",
            "\n",
            "copy data from google drive\n",
            "0: Copy files from: D1/preprocessed/\n",
            "\n",
            "The current working directory is /content/RunTimeDir\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCqk-GWaKBWA",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMQ3lIgtaArT",
        "colab_type": "code",
        "outputId": "856b5395-a82c-49c3-bcc4-fbb42b609325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "from scipy import signal\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "class DataContainer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.dataWindowArray = []\n",
        "        self.length = 0\n",
        "\n",
        "        # Normalizing values of whole trial\n",
        "        self.epocMean = -1\n",
        "        self.epocStd = -1\n",
        "        self.shimmerMean = -1\n",
        "        self.shimmerStd = -1\n",
        "\n",
        "    def createWindow(self,beginTime, endTime, shimmerSize, epocSize, shimmerScaling=None,\n",
        "                                                                     epocScaling=None):\n",
        "        self.dataWindowArray.append(DataWindow(beginTime, endTime, shimmerSize, epocSize,shimmerScaling, epocScaling))\n",
        "        self.length += 1\n",
        "\n",
        "    def fillData(self, shimmerData=None, epocData=None):\n",
        "        #Fill Shimmer Data\n",
        "        windowIdx = 0\n",
        "        shimmerIdx = 0\n",
        "        shimmerData = shimmerData[['PPG', 'COMPUTER_TIME', 'LABEL']].values\n",
        "        for i in range(shimmerData.shape[0]):\n",
        "            while shimmerData[i,-2] > self.dataWindowArray[windowIdx].endTime:\n",
        "                windowIdx += 1\n",
        "                shimmerIdx = 0\n",
        "\n",
        "                if windowIdx >= self.length:\n",
        "                    break\n",
        "\n",
        "            if windowIdx >= self.length:\n",
        "                break\n",
        "\n",
        "            self.dataWindowArray[windowIdx].shimmerArray[shimmerIdx] = shimmerData[i,:]\n",
        "            self.dataWindowArray[windowIdx].actualShimmerSize += 1\n",
        "            shimmerIdx += 1\n",
        "\n",
        "        #Fill Epoc Data\n",
        "        windowIdx = 0\n",
        "        epocIdx = 0\n",
        "        epocData = epocData[['AF3', 'F7', 'F3', 'FC5', 'T7',\n",
        "                             'P7', 'O1', 'O2', 'P8', 'T8',\n",
        "                             'FC6', 'F4', 'F8', 'AF4', 'COMPUTER_TIME',\n",
        "                             'LABEL']].values\n",
        "       \n",
        "                    \n",
        "        for i in range(epocData.shape[0]):\n",
        "          \n",
        "            '''\n",
        "            Get the correct initial window (This needs to be done because the secondary \n",
        "            task labels started before that the sensor streaming of data)\n",
        "            '''\n",
        "            while epocData[i,-2] > self.dataWindowArray[windowIdx].endTime:\n",
        "                windowIdx += 1\n",
        "                epocIdx = 0\n",
        "            \n",
        "                if windowIdx >= self.length:\n",
        "                    break\n",
        "            \n",
        "            \n",
        "            if windowIdx >= self.length:\n",
        "                break\n",
        "            \n",
        "            '''\n",
        "            Discard data that was taken before the first window of time\n",
        "            '''\n",
        "            if epocData[i,-2] > self.dataWindowArray[windowIdx].beginTime:\n",
        "                self.dataWindowArray[windowIdx].epocArray[epocIdx] = epocData[i,:]\n",
        "                self.dataWindowArray[windowIdx].actualEpocSize += 1\n",
        "                epocIdx += 1\n",
        "\n",
        "    def createMetrics(self):\n",
        "            \n",
        "        for i in range(self.length):\n",
        "            self.dataWindowArray[i].calculateLabel()\n",
        "            self.dataWindowArray[i].createSpectogramVolume()\n",
        "\n",
        "    def normalizeData(self):\n",
        "        for i in range(self.length):\n",
        "            self.dataWindowArray[i].normalizeData()\n",
        "\n",
        "    def dumpWindowsToFiles(self, destinationPath, trial):\n",
        "        for i in range(self.length):\n",
        "            self.dataWindowArray[i].createPickleFile(i, destinationPath, trial)\n",
        "\n",
        "\n",
        "\n",
        "class DataWindow:\n",
        "\n",
        "    def __init__(self, beginTime, endTime, shimmerSize, epocSize,shimmerScaling=None, epocScaling=None):\n",
        "\n",
        "        self.beginTime = float(beginTime)\n",
        "        self.endTime = float(endTime)\n",
        "\n",
        "        self.shimmerArray = np.zeros((shimmerSize, 1+2))\n",
        "        self.epocArray = np.zeros((epocSize, 14+2))\n",
        "        self.actualShimmerSize = 0\n",
        "        self.actualEpocSize = 0\n",
        "\n",
        "        self.spectogramVolume = np.zeros((14,51,7))\n",
        "        self.globalLabel = None\n",
        "\n",
        "        self.shimmerMean = shimmerScaling[0]\n",
        "        self.shimmerStd = shimmerScaling[1]\n",
        "        self.epocMean = epocScaling[0]\n",
        "        self.epocStd = epocScaling[1]\n",
        "\n",
        "\n",
        "    def createSpectogramVolume(self):\n",
        "        if self.actualEpocSize > 400:\n",
        "            #Add EEG Spectogram\n",
        "            for j in range(14):\n",
        "                x = self.epocArray[:self.actualEpocSize, j]\n",
        "                f, t, Sxx = signal.spectrogram(x, 1, nperseg=100, mode='magnitude')\n",
        "                # print(Sxx.max())\n",
        "                Sxx = cv2.resize(Sxx, dsize=(7, 51))\n",
        "                self.spectogramVolume[j, :, :] = Sxx\n",
        "\n",
        "            # #Add PPG Spectogram\n",
        "            # if self.actualShimmerSize < 600:\n",
        "            #     print(self.actualShimmerSize)\n",
        "            #\n",
        "            # x = self.shimmerArray[:self.actualShimmerSize, 0]\n",
        "            # f, t, Sxx = signal.spectrogram(x, 1, nperseg=100, mode='magnitude')\n",
        "            # Sxx = cv2.resize(Sxx, dsize=(7, 51))\n",
        "            # self.spectogramVolume[14, :, :] = Sxx\n",
        "\n",
        "\n",
        "    def normalizeData(self):\n",
        "        self.epocArray[:,:14] = (self.epocArray[:,:14] - self.epocMean)/self.epocStd\n",
        "        self.shimmerArray[:,0] = (self.shimmerArray[:,0] - self.shimmerMean)/self.shimmerStd\n",
        "\n",
        "    def calculateLabel(self):\n",
        "        totalLength = self.actualEpocSize + 1e-6\n",
        "        label = sum(self.epocArray[:self.actualEpocSize, -1]) / totalLength\n",
        "        self.globalLabel = round(label)\n",
        "\n",
        "    def createPickleFile(self, windowIdx, destinationPath, trial):\n",
        "        if self.actualEpocSize > 400 and windowIdx>4:\n",
        "            finalDict = {'data': self.spectogramVolume, 'label': self.globalLabel}\n",
        "            with open(destinationPath + 'S1_T{:d}_{:03d}.pickle'.format(trial, windowIdx), 'wb') as handle:\n",
        "                pickle.dump(finalDict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "TRIAL = 7\n",
        "DATASET = 1\n",
        "import os\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "    os.chdir('/content/RunTimeDir')\n",
        "    path = os.getcwd()  \n",
        "    print (\"The current working directory is %s\" % path, end='\\n\\n') \n",
        "\n",
        "    destinationPath = \"./Data/D{:d}/final/\".format(DATASET)\n",
        "    dataPath = \"./Data/D{:d}/preprocessed/\".format(DATASET)\n",
        "    \n",
        "    try:\n",
        "        os.mkdir(destinationPath)\n",
        "    except OSError:\n",
        "        print(\"Creation of the directory %s failed\" % destinationPath)\n",
        "    else:\n",
        "        print(\"Successfully created the directory %s \" % destinationPath)\n",
        "\n",
        "    \n",
        "    for TRIAL in [3,4,5,6,7]:\n",
        "        shimmerFile = pd.read_csv(dataPath + 'S1_T{:d}_fusion_shimmer.txt'.format(TRIAL), sep=' ')\n",
        "        epocFile = pd.read_csv(dataPath + 'S1_T{:d}_fusion_epoc.txt'.format(TRIAL), sep=' ')\n",
        "        fiveSecondWindowStamps = pd.read_csv(dataPath + \"S1_T{:d}_TimestampEvery5Seconds.txt\".format(TRIAL)\n",
        "                                             , sep=' ').values\n",
        "\n",
        "        #Get Normalizing Values\n",
        "        shimmerData = shimmerFile['PPG'].values\n",
        "        epocData = epocFile[['AF3', 'F7', 'F3', 'FC5', 'T7',\n",
        "                             'P7', 'O1', 'O2', 'P8', 'T8',\n",
        "                             'FC6', 'F4', 'F8', 'AF4']].values\n",
        "\n",
        "        epocMean, epocStd = epocData.mean(), epocData.std()\n",
        "        shimmerMean, shimmerStd = shimmerData.mean(), shimmerData.std()\n",
        "\n",
        "        #Create and fill Data Container\n",
        "        container = DataContainer()\n",
        "\n",
        "        for i in range(fiveSecondWindowStamps.shape[0] - 1):\n",
        "            container.createWindow(fiveSecondWindowStamps[i],\n",
        "                                   fiveSecondWindowStamps[i+1],\n",
        "                                   1000, 1000,\n",
        "                                   shimmerScaling=(shimmerMean, shimmerStd),\n",
        "                                   epocScaling=(epocMean,epocStd))\n",
        "\n",
        "        container.fillData(shimmerData=shimmerFile, epocData = epocFile)\n",
        "        container.normalizeData()\n",
        "        container.createMetrics()\n",
        "        container.dumpWindowsToFiles(destinationPath, TRIAL)\n",
        "\n",
        "        print(\"Finish Creating Pickle Files from trial {:d}\".format(TRIAL))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The current working directory is /content/RunTimeDir\n",
            "\n",
            "Creation of the directory ./Data/D1/final/ failed\n",
            "Finish Creating Pickle Files from trial 3\n",
            "Finish Creating Pickle Files from trial 4\n",
            "Finish Creating Pickle Files from trial 5\n",
            "Finish Creating Pickle Files from trial 6\n",
            "Finish Creating Pickle Files from trial 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hk5Npfa2Zsy",
        "colab_type": "text"
      },
      "source": [
        "#Test GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQTUePAy2dN7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "f5cc6f4f-6e60-414b-c9aa-02d234d58652"
      },
      "source": [
        "import torch\n",
        "import pycuda.driver as cuda\n",
        "\n",
        "\n",
        "print(\"Do we have GPUs available?\")\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "## Get Id of default device\n",
        "cuda.init()\n",
        "print(\"Default device: {:d}\".format(torch.cuda.current_device()) )\n",
        "print(\"GPU name: {:s}\".format(cuda.Device(0).name()))\n",
        "\n",
        "print(\"GPU name with Pytorch: {:s}\".format(torch.cuda.get_device_name(0) ))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Do we have GPUs available?\n",
            "True\n",
            "Default device: 0\n",
            "GPU name: Tesla T4\n",
            "GPU name with Pytorch: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z1BL5rWxCWa",
        "colab_type": "text"
      },
      "source": [
        "#Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nb-tgMZ1ZPg",
        "colab_type": "text"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilPowRJBxWIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ConvNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, out_1=2, out_2=1, out_3=12, activation=\"relu\"):\n",
        "\n",
        "        super(ConvNetwork,self).__init__()\n",
        "\n",
        "        #SET activation\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'selu':\n",
        "            self.activation = nn.SELU()\n",
        "        elif activation == 'elu':\n",
        "            self.activation = nn.ELU()\n",
        "\n",
        "        #First Convolutional Layer\n",
        "        self.cnn1 = nn.Conv2d(in_channels=14,out_channels=out_1,kernel_size=1,padding=0)\n",
        "        #self.maxpool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(out_1)\n",
        "        self.relu1 = self.activation\n",
        "\n",
        "        #Second Convolutional Layer\n",
        "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=3, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(out_2)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu2 = self.activation\n",
        "\n",
        "        # Third Convolutional Layer\n",
        "        self.cnn3 = nn.Conv2d(in_channels=out_2, out_channels=out_3, kernel_size=1, padding=0)\n",
        "        self.bn3 = nn.BatchNorm2d(out_3)\n",
        "        self.relu3 = self.activation\n",
        "\n",
        "        #Fully connected Layer\n",
        "        self.fc1 = nn.Linear(out_3*26*4, 784)\n",
        "        self.fc_bn = nn.BatchNorm1d(784)\n",
        "        self.fc2 = nn.Linear(784, 2)\n",
        "        self.relu4 = self.activation\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        out = self.cnn1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        #out = self.maxpool1(out)\n",
        "        out = self.cnn2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.maxpool2(out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        out = self.cnn3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu3(out)\n",
        "\n",
        "        out = out.view(out.size(0),-1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc_bn(out)\n",
        "        out = self.relu4(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHUvV76u1d3T",
        "colab_type": "text"
      },
      "source": [
        "EEG dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPooMyR_1hug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from os.path import isfile, join\n",
        "from os import listdir\n",
        "import pickle\n",
        "\n",
        "\n",
        "class EegDataset(Dataset):\n",
        "\n",
        "    def __init__(self,datasetPath=\"./Dataset/\",transform =None):\n",
        "\n",
        "        self.datasetPath = datasetPath\n",
        "        self.files = [join(datasetPath, f) for f in listdir(datasetPath) if isfile(join(datasetPath, f))]\n",
        "        self.len = len(self.files)\n",
        "\n",
        "        self.x = torch.zeros((self.len, 14, 51, 7))\n",
        "        self.y = torch.zeros(len(self.files),dtype=torch.long)\n",
        "\n",
        "        self.positiveLength = 0\n",
        "        self.negativeLength = 0\n",
        "        self.positiveIdx = []\n",
        "        self.negativeIdx = []\n",
        "\n",
        "        for i in range(len(self.files)):\n",
        "\n",
        "            with open(self.files[i],'rb') as f1:\n",
        "                dataDict = pickle.load(f1)\n",
        "                self.x[i, :, :, :] = torch.torch.from_numpy(dataDict['data'])\n",
        "                self.y[i] = int(dataDict['label'])\n",
        "\n",
        "                if self.y[i] == 1:\n",
        "                    self.positiveLength += 1\n",
        "                    self.positiveIdx.append(i)\n",
        "                elif self.y[i] == 0:\n",
        "                    self.negativeLength += 1\n",
        "                    self.negativeIdx.append(i)\n",
        "\n",
        "\n",
        "        # Normalizin Values\n",
        "        self.xMean = self.x.mean()\n",
        "        self.xStd  = self.x.std()\n",
        "        self.arbitraryScaling = self.x.max()*3/8\n",
        "        self.xMax = self.x.max()\n",
        "        self.xMin = self.x.min()\n",
        "\n",
        "        # Scaling by (max - min)\n",
        "        self.x = self.x / (self.xMax - self.xMin + 1e-7)\n",
        "\n",
        "        #Scaling by mean and std\n",
        "        #self.x = (self.x - self.xMean) / (self.xStd + 1e-7)\n",
        "\n",
        "        #Scaling with arbitrary scaling factor\n",
        "        # self.x = self.x / (self.arbitraryScaling)\n",
        "\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        sample = self.x[index], self.y[index]\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV5YSRxz1ibC",
        "colab_type": "text"
      },
      "source": [
        "Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXmo6mox1kzt",
        "colab_type": "code",
        "outputId": "9686d20c-9129-4c61-b3b4-885dca8cfd20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.optim as optim\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "def save_checkpoint(optimizer, model, epoch, trLossH, valLossH, valAccH, filename):\n",
        "    checkpoint_dict = {'optimizer': optimizer.state_dict(),\n",
        "                       'model': model.state_dict(),\n",
        "                       'epoch': epoch,\n",
        "                       'trLossH': trLossH,\n",
        "                       'valLossH': valLossH,\n",
        "                       'valAccH': valAccH}\n",
        "\n",
        "    torch.save(checkpoint_dict, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(optimizer, model, filename):\n",
        "    checkpoint_dict = torch.load(filename, map_location=DEVICE)\n",
        "    epoch = checkpoint_dict['epoch']\n",
        "    trLossH = checkpoint_dict['trLossH']\n",
        "    valLossH = checkpoint_dict['valLossH']\n",
        "    valAccH = checkpoint_dict['valAccH']\n",
        "\n",
        "    model.load_state_dict(checkpoint_dict['model'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "\n",
        "    return epoch, trLossH, valLossH, valAccH\n",
        "\n",
        "\n",
        "def fiveKFoldTraining(trainData, folds, cnn, totalEpochs, modelNumb):\n",
        "\n",
        "    valAccuracyPlots = []\n",
        "    loss = 0\n",
        "    for i in range(5):\n",
        "        print(\"Start Training Fold {:d}\".format(i))\n",
        "        #Create Data loaders\n",
        "        trainLoader = DataLoader(trainData, batch_size=16, sampler=folds[i]['train'])\n",
        "        validLoader = DataLoader(trainData, batch_size=16, sampler=folds[i]['val'])\n",
        "\n",
        "        #Create the model, optimizer and loss function\n",
        "        torch.manual_seed(RANDOM_SEED)\n",
        "        model = ConvNetwork( out_1=cnn['CONV_LAYER1'],\n",
        "                             out_2=cnn['CONV_LAYER2'],\n",
        "                             out_3=cnn['CONV_LAYER3'],\n",
        "                             activation=cnn['ACTIVATION'])\n",
        "        \n",
        "        model.to(DEVICE)\n",
        "        lossFunction = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(params=model.parameters(), lr=0.01, weight_decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "        #TrainLogger\n",
        "        valAccuracyHistory = []\n",
        "        valLossHistory = []\n",
        "        trLossHistory = []\n",
        "\n",
        "        #Train Fold\n",
        "        #Reset Max accuracy\n",
        "        maxValidationAccuracy = 0.7\n",
        "\n",
        "        for epoch in range(totalEpochs):\n",
        "          \n",
        "            #Train Model with training set\n",
        "            model.train()\n",
        "            for x, y in trainLoader:\n",
        "                x, y = x.to(DEVICE), y.to(DEVICE)  \n",
        "                optimizer.zero_grad()\n",
        "                yHat = model(x)\n",
        "                loss = lossFunction(yHat, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                trLossHistory.append(loss.item())\n",
        "\n",
        "            \n",
        "            #Calculate Accuracy with validation set\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            \n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for x, y in validLoader:\n",
        "                    x, y = x.to(DEVICE), y.to(DEVICE)  \n",
        "                    z = model(x)\n",
        "                    yHat = z.argmax(1)\n",
        "                    correct += (yHat == y).sum().item()\n",
        "                    total += y.shape[0]\n",
        "\n",
        "            accuracy = correct / total\n",
        "            valAccuracyHistory.append(accuracy)\n",
        "            \n",
        "            #Print Accuracy\n",
        "            if epoch%10 == 0 and epoch != 0:\n",
        "                #print(\"Loss in Training in epoch {:d} set:       {:.5f}\".format(epoch, loss))\n",
        "                print(\"Max validation accuracy in epoch {:d} set: {:.5f}\".format(epoch, max(valAccuracyHistory)))\n",
        "\n",
        "            # Save Check Point\n",
        "            if accuracy > maxValidationAccuracy:\n",
        "                maxValidationAccuracy = accuracy\n",
        "                checkpoint_filename = MODELS_DIRECTORY + 'Model{:d}/classifier-f{:02d}-e{:03d}.pkl'.format(modelNumb, i, epoch)\n",
        "                save_checkpoint(optimizer, model, epoch, trLossHistory, valLossHistory, valAccuracyHistory,\n",
        "                                checkpoint_filename)\n",
        "\n",
        "                checkpoint_txt = MODELS_DIRECTORY + 'Model{:d}/classifier-f{:02d}-e{:03d}-acc{:0.5f}.txt'.format(modelNumb, i, epoch, maxValidationAccuracy)\n",
        "                with open(checkpoint_txt,'w') as f:\n",
        "                    pass\n",
        "\n",
        "                maxValidationAccuracy = accuracy\n",
        "\n",
        "        maxValAccPerFold =  max(valAccuracyHistory)\n",
        "        valAccuracyPlots.append(valAccuracyHistory)\n",
        "        print(\"Max Validation Accuracy fold {:02d}:        {:0.5f}\\n\".format(i, maxValAccPerFold))\n",
        "\n",
        "\n",
        "    #Save data Accuracies\n",
        "    statsFile = MODELS_DIRECTORY + 'Model{:d}/foldAccuracies.pkl'.format(modelNumb)\n",
        "    with open(statsFile, 'wb') as f:\n",
        "        pickle.dump(valAccuracyPlots, f)\n",
        "\n",
        "    #Print training summary\n",
        "    for i in range(5):\n",
        "        print(\"Max accuracy in fold {:d}: {:.4f}\".format(i,max(valAccuracyPlots[i])))\n",
        "\n",
        "    averageMaxAcc = sum([max(valAccuracyPlots[i]) for i in range(5)])/5\n",
        "    print(\"Average Max Accuracy: {:.5f}\".format(averageMaxAcc))\n",
        "\n",
        "\n",
        "def createModelDirectory(modelNumber):\n",
        "    path = MODELS_DIRECTORY + \"Model{:02d}\".format(modelNumber)\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError:\n",
        "        print(\"Creation of the directory %s failed\" % path)\n",
        "    else:\n",
        "        print(\"Successfully created the directory %s \" % path)\n",
        "\n",
        "\n",
        "#Cnn Architectures -- Best Model cnn8!!\n",
        "cnn1 = {\"CONV_LAYER1\": 32, \"CONV_LAYER2\": 64, \"CONV_LAYER3\": 12, \"ACTIVATION\": 'selu'}\n",
        "cnn2 = {\"CONV_LAYER1\": 32, \"CONV_LAYER2\": 64, \"CONV_LAYER3\": 12, \"ACTIVATION\": 'elu'}\n",
        "cnn3 = {\"CONV_LAYER1\": 32, \"CONV_LAYER2\": 64, \"CONV_LAYER3\": 12, \"ACTIVATION\": 'relu'}\n",
        "cnn4 = {\"CONV_LAYER1\": 16, \"CONV_LAYER2\": 32, \"CONV_LAYER3\": 6, \"ACTIVATION\": 'selu'}\n",
        "cnn5 = {\"CONV_LAYER1\": 16, \"CONV_LAYER2\": 32, \"CONV_LAYER3\": 6, \"ACTIVATION\": 'elu'}\n",
        "cnn6 = {\"CONV_LAYER1\": 16, \"CONV_LAYER2\": 32, \"CONV_LAYER3\": 6, \"ACTIVATION\": 'relu'}\n",
        "cnn7 = {\"CONV_LAYER1\": 64, \"CONV_LAYER2\": 128, \"CONV_LAYER3\": 24, \"ACTIVATION\": 'selu'}\n",
        "cnn8 = {\"CONV_LAYER1\": 64, \"CONV_LAYER2\": 128, \"CONV_LAYER3\": 24, \"ACTIVATION\": 'elu'}\n",
        "cnn9 = {\"CONV_LAYER1\": 64, \"CONV_LAYER2\": 128, \"CONV_LAYER3\": 24, \"ACTIVATION\": 'relu'}\n",
        "\n",
        "\n",
        "#Global variables\n",
        "DEVICE = 'cuda'\n",
        "MODELS_DIRECTORY = './Models/'\n",
        "\n",
        "#RandomSeed\n",
        "#Model 2 and Model 3\n",
        "#RANDOM_SEED = 742\n",
        "#Model 4\n",
        "RANDOM_SEED = 480\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    #Create Training data object\n",
        "    trainData = EegDataset(datasetPath=\"./Data/D1/final\")\n",
        "    print(\"Number of negative examples \",trainData.negativeLength)\n",
        "    print(\"Number of positive examples \", trainData.positiveLength)\n",
        "\n",
        "    #Shuffle training data\n",
        "    split = int(0.2 * len(trainData))\n",
        "    np.random.seed(seed=RANDOM_SEED)\n",
        "    idxArr = np.random.permutation(len(trainData))\n",
        "\n",
        "    #Create 5 folds for cross Validation\n",
        "    folds = []\n",
        "    total = len(trainData)\n",
        "\n",
        "    for i in range(5):\n",
        "        valRange = range(split*i, split*(i+1) if split*(i+1) < total else total)\n",
        "        valIdx = idxArr[valRange]\n",
        "        trainIdx = np.delete(idxArr, valRange)\n",
        "        trainSampler = SubsetRandomSampler(trainIdx)\n",
        "        valSampler = SubsetRandomSampler(valIdx)\n",
        "        folds.append({'train': trainSampler, 'val': valSampler})\n",
        "\n",
        "    #Train Models\n",
        "    totalEpochs = 50\n",
        "    print(\"Start training\")\n",
        "    #cnnArch = [cnn1,cnn2,cnn3,cnn4,cnn5,cnn6,cnn7,cnn8,cnn9]\n",
        "    cnnArch = [cnn8]\n",
        "    modelNumber = 31\n",
        "\n",
        "    for cnn in cnnArch:\n",
        "        print(\"Start Training Model {:d}\".format(modelNumber))\n",
        "        createModelDirectory(modelNumber)\n",
        "        fiveKFoldTraining(trainData, folds, cnn, totalEpochs, modelNumber)\n",
        "        modelNumber += 1\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of negative examples  155\n",
            "Number of positive examples  164\n",
            "Start training\n",
            "Start Training Model 31\n",
            "Creation of the directory ./Models/Model31 failed\n",
            "Start Training Fold 0\n",
            "Max validation accuracy in epoch 10 set: 0.69841\n",
            "Max validation accuracy in epoch 20 set: 0.69841\n",
            "Max validation accuracy in epoch 30 set: 0.71429\n",
            "Max validation accuracy in epoch 40 set: 0.76190\n",
            "Max Validation Accuracy fold 00:        0.76190\n",
            "\n",
            "Start Training Fold 1\n",
            "Max validation accuracy in epoch 10 set: 0.76190\n",
            "Max validation accuracy in epoch 20 set: 0.80952\n",
            "Max validation accuracy in epoch 30 set: 0.80952\n",
            "Max validation accuracy in epoch 40 set: 0.80952\n",
            "Max Validation Accuracy fold 01:        0.80952\n",
            "\n",
            "Start Training Fold 2\n",
            "Max validation accuracy in epoch 10 set: 0.69841\n",
            "Max validation accuracy in epoch 20 set: 0.79365\n",
            "Max validation accuracy in epoch 30 set: 0.80952\n",
            "Max validation accuracy in epoch 40 set: 0.80952\n",
            "Max Validation Accuracy fold 02:        0.80952\n",
            "\n",
            "Start Training Fold 3\n",
            "Max validation accuracy in epoch 10 set: 0.71429\n",
            "Max validation accuracy in epoch 20 set: 0.71429\n",
            "Max validation accuracy in epoch 30 set: 0.73016\n",
            "Max validation accuracy in epoch 40 set: 0.73016\n",
            "Max Validation Accuracy fold 03:        0.74603\n",
            "\n",
            "Start Training Fold 4\n",
            "Max validation accuracy in epoch 10 set: 0.76190\n",
            "Max validation accuracy in epoch 20 set: 0.77778\n",
            "Max validation accuracy in epoch 30 set: 0.77778\n",
            "Max validation accuracy in epoch 40 set: 0.77778\n",
            "Max Validation Accuracy fold 04:        0.77778\n",
            "\n",
            "Max accuracy in fold 0: 0.7619\n",
            "Max accuracy in fold 1: 0.8095\n",
            "Max accuracy in fold 2: 0.8095\n",
            "Max accuracy in fold 3: 0.7460\n",
            "Max accuracy in fold 4: 0.7778\n",
            "Average Max Accuracy: 0.78095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0MDmu6C9IeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}